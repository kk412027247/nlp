{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generating text.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPumeM48VUPu5Ul6kZ2klcY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kk412027247/nlp/blob/main/generating_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmfhCnioEJ_Q"
      },
      "outputs": [],
      "source": [
        "!wget https://www.wangluoguimi.com/news/news-headlines.tsv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -3 news-headlines.tsv"
      ],
      "metadata": {
        "id": "l2x5Tf5ZSaZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "chars = sorted(set(\"abcdefghijklmnopqrstuvwxyz0123456789 -,;.!?:’’’/\\|_@#$%ˆ&*˜‘+-=()[]{}' ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\n",
        "chars = list(chars)\n",
        "EOS = 'EOS'\n",
        "UNK = '<UNK>'\n",
        "PAD = '<PAD>'\n",
        "chars.append(UNK)\n",
        "chars.append(EOS)\n",
        "chars.insert(0, PAD)\n",
        "\n",
        "char2idx = {u: i for i, u in enumerate(chars)}\n",
        "idx2char =  np.array(chars)\n",
        "\n",
        "def char_idx(c):\n",
        "  if c in chars:\n",
        "    return char2idx[c]\n",
        "  return char2idx[UNK]\n",
        "\n",
        "data = []\n",
        "MAX_LEN = 75\n",
        "\n",
        "with open('news-headlines.tsv', 'r') as file:\n",
        "  lines = csv.reader(file, delimiter='\\t')\n",
        "  for line in lines:\n",
        "    hdln = line[0]\n",
        "    cnvrtd = [char_idx(c) for c in hdln[:-1]]\n",
        "    if len(cnvrtd) >= MAX_LEN:\n",
        "      cnvrtd = cnvrtd[0:MAX_LEN-1]\n",
        "      cnvrtd.append(char2idx[EOS])\n",
        "    else:\n",
        "      cnvrtd.append(char2idx[EOS])\n",
        "      remain=MAX_LEN - len(cnvrtd)\n",
        "      if remain > 0:\n",
        "        for i in range(remain):\n",
        "          cnvrtd.append(char2idx[PAD])\n",
        "    data.append(cnvrtd)\n",
        "print('Data file loaded')"
      ],
      "metadata": {
        "id": "yhPfRRCFUzEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_data = np.array(data)\n",
        "np_data_in = np_data[:, :-1]\n",
        "np_data_out = np_data[:, 1:]\n",
        "np_data_in\n",
        "import tensorflow as tf\n",
        "x = tf.data.Dataset.from_tensor_slices((np_data_in, np_data_out))"
      ],
      "metadata": {
        "id": "xPDvwRTTaH2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "x_train= x.shuffle(100000, reshuffle_each_iteration=True).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True, batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(vocab_size=vocab_size, embedding_dim=embedding_dim, rnn_units=rnn_units,batch_size=BATCH_SIZE)\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "LbVtf5DRbkUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer = 'adam', loss= loss)"
      ],
      "metadata": {
        "id": "SKfO3wg-m0D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import os\n",
        "dt = datetime.datetime.today().strftime(\"%Y-%b-%d-%H-%M-%S\")\n",
        "checkpoint_dir = './training_checkpoints'+dt\n",
        "checkpoint_prefx = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefx, save_weights_only=True)"
      ],
      "metadata": {
        "id": "0KVHDbyQnkUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "EPOCHS=25\n",
        "start = time.time()\n",
        "history = model.fit(x_train, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "8kEL8RNJowzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "lossplot = 'loss-'+dt+'.png'\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.savefig(lossplot)\n",
        "print('Save loss to: ', lossplot)"
      ],
      "metadata": {
        "id": "DX9KlMz3pM2i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}