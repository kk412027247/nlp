{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMS_Spam_detection.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPOxr3DutcYYWwhiVAE/Tpr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kk412027247/nlp/blob/main/SMS_Spam_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLTTiHNO30KM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsR4bSFykAK0"
      },
      "source": [
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import re\n",
        "tf.__version__\n",
        "\n",
        "!pip install stopwordsiso\n",
        "!pip install stanfordnlp\n",
        "!pip install stanza\n",
        "# !pip install git+git://github.com/stanfordnlp/stanza.git@dev\n",
        "\n",
        "\n",
        "import stanfordnlp as snlp\n",
        "import stopwordsiso as stopwords\n",
        "import stanza\n",
        "\n",
        "\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\"smsspamcollection.zip\",\n",
        "                                      origin = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\",extract=True)\n",
        "!unzip $path_to_zip -d data\n",
        "en = snlp.download('en')\n",
        "en = stanza.download('en')\n",
        "\n",
        "\n",
        "\n",
        "lines = io.open('data/SMSSpamCollection').read().strip().split('\\n')\n",
        "lines[0]\n",
        "\n",
        "spam_dataset = []\n",
        "for line in lines:\n",
        "  label, text = line.split('\\t')\n",
        "  if label.lower().strip() == 'spam':\n",
        "    spam_dataset.append((1, text.strip()))\n",
        "  else:\n",
        "    spam_dataset.append(((0, text.strip())))\n",
        "print(spam_dataset[0])\n",
        "\n",
        "df = pd.DataFrame(spam_dataset, columns=['Spam', 'Message'])\n",
        "\n",
        "def message_lenth(x):\n",
        "  return len(x)\n",
        "\n",
        "def num_capitals(x):\n",
        "  _, count = re.subn(r'[A-Z]', '', x)\n",
        "  return count\n",
        "\n",
        "def num_puntuation(x):\n",
        "  _,count = re.subn(r'\\W', '', x)\n",
        "  return count\n",
        "\n",
        "df['Capitals'] = df['Message'].apply(num_capitals)\n",
        "df['Punctuation'] = df['Message'].apply(num_puntuation)\n",
        "df['Length'] = df['Message'].apply(message_lenth)\n",
        "df.describe()\n",
        "\n",
        "train=df.sample(frac=0.8, random_state=42)\n",
        "test=df.drop(train.index)\n",
        "x_train=train[['Length', 'Capitals', 'Punctuation']]\n",
        "y_train = train[['Spam']]\n",
        "x_test = test[['Length', 'Capitals', 'Punctuation']]\n",
        "y_test = test[['Spam']]\n",
        "\n",
        "\n",
        "def make_model(input_dims=3, num_units=12):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(num_units,\n",
        "                                  input_dim=input_dims,\n",
        "                                  activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = make_model()\n",
        "# model.fit(x_train, y_train, epochs=10, batch_size=10)\n",
        "\n",
        "# model.evaluate(x_test, y_test)\n",
        "\n",
        "# y_train_pred = model.predict(x_train)\n",
        "# x = tf.math.confusion_matrix(tf.constant(y_train.Spam), y_train_pred)\n",
        "# print(x)\n",
        "\n",
        "sentence = 'Go until jurong point, crazy.. Available only in bugis n great world'\n",
        "sentence.split()\n",
        "\n",
        "\n",
        "\n",
        "# en = snlp.Pipeline(lang='en', processors='tokenize')\n",
        "\n",
        "# tokenized = en(sentence)\n",
        "# len(tokenized.sentences)\n",
        "\n",
        "# for snt in tokenized.sentences:\n",
        "#   for word in snt.tokens:\n",
        "#     print(word.text)\n",
        "#   print(\"<end of Sentence>\")\n",
        "\n",
        "\n",
        "\n",
        "# jp = snlp.download('ja')\n",
        "# jp = snlp.Pipeline(lang='ja', processors='tokenize')\n",
        "# jp_line = jp(\"選挙管理委員会\")\n",
        "\n",
        "# for snt in jp_line.sentences:\n",
        "#   for word in snt.tokens:\n",
        "#     print(word.text)\n",
        "\n",
        "\n",
        "\n",
        "# pipeline = snlp.Pipeline(lang='en', processors='tokenize')\n",
        "en_sw = stopwords.stopwords('en')\n",
        "\n",
        "en = stanza.Pipeline(lang='en')\n",
        "\n",
        "\n",
        "def word_counts_v3(x, pipeline=en):\n",
        "  totals = 0.\n",
        "  count = 0.\n",
        "  non_word = 0.\n",
        "  try:\n",
        "    doc = pipeline(x)\n",
        "    for sentence in doc.sentences:\n",
        "      totals += len(sentence.tokens)  # (1)\n",
        "      for token in sentence.tokens:\n",
        "          if token.text.lower() not in en_sw:\n",
        "            if token.words[0].upos not in ['PUNCT', 'SYM']:\n",
        "              count += 1.\n",
        "            else:\n",
        "              non_word += 1.\n",
        "    non_word = non_word / totals\n",
        "    return pd.Series([count, non_word], index=['Words_NoPunct', 'Punct'])\n",
        "  except:\n",
        "    print(x)\n",
        "    return pd.Series([count, non_word], index=['Words_NoPunct', 'Punct'])\n",
        "\n",
        "train_tmp = train['Message'].apply(word_counts_v3)\n",
        "train = pd.concat(['train, train_tmp'], axis=1)\n",
        "\n",
        "test_tmp = test['Message'].apply(word_counts_v3)\n",
        "test = pd.concat(['test, test_tmp'], axis=1)\n",
        "\n",
        "\n",
        "print(train.loc[train.Spam == 1].describe())\n",
        "print(train.loc[train.Spam == 0].describe())\n",
        "\n",
        "\n",
        "x_train=train[['Length', 'Capitals', 'Punctuation', 'Words_NoPunct', 'Punt']]\n",
        "y_train = train[['Spam']]\n",
        "x_test = test[['Length', 'Capitals', 'Punctuation', 'Words_NoPunct', 'Punt']]\n",
        "y_test = test[['Spam']]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = make_model(input_dims=5)\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=10)\n",
        "\n",
        "model.evaluate(x_test, y_test)\n",
        "\n",
        "y_train_pred = model.predict(x_train)\n",
        "x = tf.math.confusion_matrix(tf.constant(y_train.Spam), y_train_pred)\n",
        "print(x)\n",
        "\n",
        "en = stanza.Pipeline(lang='en')\n",
        "txt = \"Yo you around? Afriend of mine's looking\"\n",
        "pos = en(txt)\n",
        "\n",
        "def print_pos(doc):\n",
        "  text = ''\n",
        "  for sentence in doc.sentences:\n",
        "    for token in sentence.tokens:\n",
        "      text += token.words[0].text + '/' + token.words[0].upos + ' '\n",
        "    text +='\\n'\n",
        "  return text\n",
        "\n",
        "print(print_pos(pos))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew25vNAE4FHl"
      },
      "source": [
        "text = \"Stemming is aimed at reducing vocabulary and aid un-derstanding of morphological processes. This helps people un-derstand the morphology of words and reduce size of corpus.\"\n",
        "en = stanza.Pipeline(lang='en')\n",
        "lemma = en(text)\n",
        "\n",
        "lemmas = ''\n",
        "for sentence in lemma.sentences:\n",
        "  for token in sentence.tokens:\n",
        "    lemmas += token.words[0].lemma + '/' + token.words[0].upos + ' '\n",
        "  lemmas += '\\n'\n",
        "print(lemmas)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}