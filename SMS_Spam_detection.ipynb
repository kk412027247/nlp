{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMS_Spam_detection.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNPxKr4FZD4lWS1GhLCrUI6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kk412027247/nlp/blob/main/SMS_Spam_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsR4bSFykAK0"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import re\n",
        "tf.__version__\n",
        "\n",
        "# path_to_zip = tf.keras.utils.get_file(\"smsspamcollection.zip\",\n",
        "#                                       origin = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\",extract=True)\n",
        "\n",
        "\n",
        "# !unzip $path_to_zip -d data\n",
        "\n",
        "lines = io.open('data/SMSSpamCollection').read().strip().split('\\n')\n",
        "lines[0]\n",
        "\n",
        "spam_dataset = []\n",
        "for line in lines:\n",
        "  label, text = line.split('\\t')\n",
        "  if label.lower().strip() == 'spam':\n",
        "    spam_dataset.append((1, text.strip()))\n",
        "  else:\n",
        "    spam_dataset.append(((0, text.strip())))\n",
        "print(spam_dataset[0])\n",
        "\n",
        "df = pd.DataFrame(spam_dataset, columns=['Spam', 'Message'])\n",
        "\n",
        "def message_lenth(x):\n",
        "  return len(x)\n",
        "\n",
        "def num_capitals(x):\n",
        "  _, count = re.subn(r'[A-Z]', '', x)\n",
        "  return count\n",
        "\n",
        "def num_puntuation(x):\n",
        "  _,count = re.subn(r'\\W', '', x)\n",
        "  return count\n",
        "\n",
        "df['Capitals'] = df['Message'].apply(num_capitals)\n",
        "df['Punctuation'] = df['Message'].apply(num_puntuation)\n",
        "df['Length'] = df['Message'].apply(message_lenth)\n",
        "df.describe()\n",
        "\n",
        "train=df.sample(frac=0.8, random_state=42)\n",
        "test=df.drop(train.index)\n",
        "x_train=train[['Length', 'Capitals', 'Punctuation']]\n",
        "y_train = train[['Spam']]\n",
        "x_test = test[['Length', 'Capitals', 'Punctuation']]\n",
        "y_test = test[['Spam']]\n",
        "\n",
        "\n",
        "def make_model(input_dims=3, num_units=12):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(num_units,\n",
        "                                  input_dim=input_dims,\n",
        "                                  activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = make_model()\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=10)\n",
        "\n",
        "model.evaluate(x_test, y_test)\n",
        "\n",
        "y_train_pred = model.predict(x_train)\n",
        "x = tf.math.confusion_matrix(tf.constant(y_train.Spam), y_train_pred)\n",
        "print(x)\n",
        "\n",
        "sentence = 'Go until jurong point, crazy.. Available only in bugis n great world'\n",
        "sentence.split()\n",
        "\n",
        "# !pip install stanfordnlp\n",
        "import stanfordnlp as snlp\n",
        "# en = snlp.download('en')\n",
        "\n",
        "# en = snlp.Pipeline(lang='en', processors='tokenize')\n",
        "\n",
        "# tokenized = en(sentence)\n",
        "# len(tokenized.sentences)\n",
        "\n",
        "# for snt in tokenized.sentences:\n",
        "#   for word in snt.tokens:\n",
        "#     print(word.text)\n",
        "#   print(\"<end of Sentence>\")\n",
        "\n",
        "\n",
        "\n",
        "# jp = snlp.download('ja')\n",
        "# jp = snlp.Pipeline(lang='ja', processors='tokenize')\n",
        "# jp_line = jp(\"選挙管理委員会\")\n",
        "\n",
        "# for snt in jp_line.sentences:\n",
        "#   for word in snt.tokens:\n",
        "#     print(word.text)\n",
        "\n",
        "\n",
        "\n",
        "pipeline = snlp.Pipeline(lang='en', processors='tokenize')\n",
        "\n",
        "def word_counts(x):\n",
        "  doc = pipeline(x)\n",
        "  count = sum( [ len(sentence.tokens) for sentence in doc.sentences] )\n",
        "  return count\n",
        "\n",
        "\n",
        "\n",
        "train['Words'] = train['Message'].apply(word_counts)\n",
        "test['Words'] = test['Message'].apply(word_counts)\n",
        "\n",
        "x_train=train[['Length', 'Capitals', 'Punctuation', 'Words']]\n",
        "y_train = train[['Spam']]\n",
        "x_test = test[['Length', 'Capitals', 'Punctuation', 'Words']]\n",
        "y_test = test[['Spam']]\n",
        "\n",
        "model = make_model(input_dims=4)\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=10)\n",
        "\n",
        "print(train.loc[train.Spam == 1].describe())\n",
        "\n",
        "\n",
        "print(train.loc[train.Spam == 0].describe())\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}