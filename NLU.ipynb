{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLU",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMhLcAWNUmb8MQVm4G7dZsA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kk412027247/nlp/blob/main/NLU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvxfM3CioqsE"
      },
      "source": [
        "# !pip install tensorflow_datasets\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "\n",
        "imdb_train, ds_info = tfds.load(name='imdb_reviews', split='train', with_info=True, as_supervised=True)\n",
        "imdb_test = tfds.load(name='imdb_reviews', split='test', as_supervised=True)\n",
        "\n",
        "\n",
        "tokenizer = tfds.deprecated.text.Tokenizer()\n",
        "\n",
        "\n",
        "# vocabulary_set = set()\n",
        "# MAX_TOKENS = 0\n",
        "# for example, label in imdb_train:\n",
        "#   some_tokens = tokenizer.tokenize(example.numpy())\n",
        "#   if MAX_TOKENS < len(some_tokens):\n",
        "#     MAX_TOKENS = len(some_tokens)\n",
        "#   vocabulary_set.update(some_tokens)\n",
        "\n",
        "# imdb_encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary_set, tokenizer=tokenizer)\n",
        "# imdb_encoder.save_to_file('reviews_vocab')\n",
        "\n",
        "imdb_encoder = tfds.deprecated.text.TokenTextEncoder.load_from_file('reviews_vocab')\n",
        "# message = imdb_encoder.decode(imdb_encoder.encode('Good case. Excellent value'))\n",
        "# print(message)\n",
        "# vocab_size = imdb_encoder.vocab_size\n",
        "# print(vocab_size, MAX_TOKENS)\n",
        "\n",
        "\n",
        "# for example, label in imdb_train.take(1):\n",
        "#   print(example)\n",
        "#   encoded = imdb_encoder.encode(example.numpy())\n",
        "#   print('encoded', encoded)\n",
        "#   print(imdb_encoder.decode(encoded))\n",
        "\n",
        "def encode_pad_transform(sample):\n",
        "  encoded = imdb_encoder.encode(sample.numpy())\n",
        "  pad = sequence.pad_sequences([encoded], padding='post', maxlen=150)\n",
        "  return np.array(pad[0], dtype=np.int64)\n",
        "\n",
        "def encode_tf_fn(sample, label):\n",
        "  encoded = tf.py_function(encode_pad_transform, inp=[sample], Tout=(tf.int64))\n",
        "  encoded.set_shape([None])\n",
        "  label.set_shape([])\n",
        "  return encoded, label\n",
        "\n",
        "# subset = imdb_train.take(10)\n",
        "# tst = subset.map(encode_tf_fn)\n",
        "# for review, label in tst.take(1):\n",
        "#   print('review', review)\n",
        "#   print('label', label)\n",
        "#   print(imdb_encoder.decode(review))\n",
        "\n",
        "encoded_train = imdb_train.map(encode_tf_fn)\n",
        "encoded_test = imdb_test.map(encode_tf_fn)\n",
        "\n",
        "def build_model_lstm(vocab_size, embedding_dim, rnn_unites, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True, batch_input_shape=[batch_size, None]),\n",
        "    # tf.keras.layers.LSTM(rnn_unites),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_unites)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "vocab_size = imdb_encoder.vocab_size\n",
        "embedding_dim = 64\n",
        "rnn_unites = 64\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "model = build_model_lstm(\n",
        "  vocab_size = vocab_size,\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_unites = rnn_unites,\n",
        "  batch_size = BATCH_SIZE\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'Precision', 'Recall'])\n",
        "encoded_train_batched=encoded_train.batch(BATCH_SIZE)\n",
        "\n",
        "model.fit(encoded_train_batched, epochs=10)\n",
        "\n",
        "model.evaluate(encoded_test.batch(BATCH_SIZE))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGU4ZYWmwBfW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}