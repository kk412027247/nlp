{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "name entity recognition.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMZKHUwvhW29et6HUfMho/n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kk412027247/nlp/blob/main/name_entity_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeUgndF7Y1wt"
      },
      "source": [
        "!wget https://gmb.let.rug.nl/releases/gmb-2.2.0.zip\n",
        "!unzip gmb-2.2.0.zip\n",
        "!mkdir ner\n",
        "!pip install tensorflow_addons==0.11.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMWpNon4N9L3"
      },
      "source": [
        "import os\n",
        "data_root = './gmb-2.2.0/data/'\n",
        "fnames = []\n",
        "\n",
        "for root, dirs, files in os.walk(data_root):\n",
        "  for filename in files:\n",
        "    if filename.endswith('.tags'):\n",
        "      fnames.append(os.path.join(root, filename))\n",
        "\n",
        "fnames[:2]\n",
        "\n",
        "\n",
        "\n",
        "import csv\n",
        "import collections\n",
        "\n",
        "ner_tags = collections.Counter()\n",
        "iob_tags = collections.Counter()\n",
        "\n",
        "def strip_ner_subcat(tag):\n",
        "  return tag.split('-')[0]\n",
        "\n",
        "def iob_format(ners):\n",
        "  iob_tokens = []\n",
        "  for idx, token in enumerate(ners):\n",
        "    if token != 'O':\n",
        "      if idx == 0:\n",
        "        token = 'B-' + token\n",
        "      elif ners[idx-1] == token:\n",
        "        token = 'I-' + token\n",
        "      else:\n",
        "        token = 'B-' + token\n",
        "    iob_tokens.append(token)\n",
        "    iob_tags[token] += 1\n",
        "  return iob_tokens\n",
        "\n",
        "total_sentences = 0\n",
        "outfiles=[]\n",
        "\n",
        "for idx, file in enumerate(fnames):\n",
        "  with open(file, 'rb') as content:\n",
        "    data = content.read().decode('utf-8').strip()\n",
        "    sentences = data.split('\\n\\n')\n",
        "    # print(idx, file, len(sentences))\n",
        "    total_sentences += len(sentences)\n",
        "\n",
        "    with open('./ner/'+str(idx)+'-'+os.path.basename(file), 'w') as outfile:\n",
        "      outfiles.append('./ner/'+str(idx)+'-'+os.path.basename(file))\n",
        "      writer = csv.writer(outfile)\n",
        "\n",
        "      for sentence in sentences:\n",
        "        toks = sentence.split('\\n')\n",
        "        words, pos, ner = [], [],[]\n",
        "        for tok in toks:\n",
        "          t = tok.split('\\t')\n",
        "          words.append(t[0])\n",
        "          pos.append(t[1])\n",
        "          ner_tags[t[3]] +=1\n",
        "          ner.append(strip_ner_subcat(t[3]))\n",
        "        writer.writerow([' '.join(words),\n",
        "                        ' '.join(iob_format(ner)),\n",
        "                        ' '.join(pos)])\n",
        "    \n",
        "print('total number of sentences: ', total_sentences)\n",
        "print(ner_tags)\n",
        "print(iob_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTcSfNnKSzv4"
      },
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import tensorflow as tf\n",
        "\n",
        "files = glob.glob('./ner/*.tags')\n",
        "data_pd = pd.concat([pd.read_csv(f, header=None, names=['text', 'label', 'pos']) for f in files ], ignore_index=True)\n",
        "data_pd.info()\n",
        "\n",
        "text_tok = Tokenizer(filters='[\\\\]^\\t\\n', lower=False, split=' ', oov_token='<OOV>')\n",
        "pos_tok = Tokenizer(filters='\\t\\n', lower=False, split=' ', oov_token='<OOV>')\n",
        "ner_tok = Tokenizer(filters='\\t\\n', lower=False, split=' ', oov_token='<OOV>')\n",
        "\n",
        "text_tok.fit_on_texts(data_pd['text'])\n",
        "pos_tok.fit_on_texts(data_pd['pos'])\n",
        "ner_tok.fit_on_texts(data_pd['label'])\n",
        "\n",
        "ner_config = ner_tok.get_config()\n",
        "text_config = text_tok.get_config()\n",
        "\n",
        "print(ner_config)\n",
        "\n",
        "text_vocab = eval(text_config['index_word'])\n",
        "ner_vocab = eval(ner_config['index_word'])\n",
        "print('unique words in vocab:', len(text_vocab))\n",
        "print('unique worner tags  in vocab:', len(ner_vocab))\n",
        "\n",
        "x_tok = text_tok.texts_to_sequences(data_pd['text'])\n",
        "y_tok = ner_tok.texts_to_sequences(data_pd['label'])\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "max_len = 50\n",
        "\n",
        "x_pad = sequence.pad_sequences(x_tok, padding='post', maxlen=max_len)\n",
        "y_pad = sequence.pad_sequences(y_tok, padding='post', maxlen=max_len)\n",
        "\n",
        "print(x_pad.shape, y_pad.shape)\n",
        "\n",
        "num_classes = len(ner_vocab) + 1\n",
        "Y = tf.keras.utils.to_categorical(y_pad, num_classes=num_classes)\n",
        "Y.shape\n",
        "\n",
        "vocab_size = len(text_vocab) + 1\n",
        "embedding_dim = 64\n",
        "rnn_units = 100\n",
        "BATCH_SIZE=90\n",
        "num_classes = len(ner_vocab) + 1\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, TimeDistributed, Dense \n",
        "dropout = 0.2\n",
        "\n",
        "def build_model_bilstm(vocab_size, embedding_dim, rnn_units, batch_size, classes):\n",
        "  model = tf.keras.Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
        "              batch_input_shape=[batch_size, None]),\n",
        "    Bidirectional(LSTM(units=rnn_units,\n",
        "                       return_sequences=True,\n",
        "                       dropout=dropout,\n",
        "                       kernel_initializer=tf.keras.initializers.he_normal())),\n",
        "    TimeDistributed(Dense(rnn_units, activation='relu')),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model_bilstm(vocab_size=vocab_size, embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE, classes=num_classes)\n",
        "model.summary()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "X = x_pad\n",
        "total_sentences = 62010\n",
        "test_size = round(total_sentences/BATCH_SIZE *0.2)\n",
        "X_train = X[BATCH_SIZE*test_size:]\n",
        "Y_train = Y[BATCH_SIZE*test_size:]\n",
        "X_test = X[0:BATCH_SIZE*test_size]\n",
        "Y_test = Y[0:BATCH_SIZE*test_size]\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=15)\n",
        "\n",
        "model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfhV2j37wQvU"
      },
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import Model, Input, Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed\n",
        "from tensorflow.keras.layers import Dropout, Bidirectional\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "class CRFLayer(Layer):\n",
        "  def __init__(self, label_size, mask_id=0, trans_params=None, name='crf', **kwargs):\n",
        "    super(CRFLayer, self).__init__(name=name, **kwargs)\n",
        "    self.label_size = label_size\n",
        "    self.mask_id = mask_id\n",
        "    self.transition_params = None\n",
        "    if trans_params is None:\n",
        "      self.transition_params = tf.Variable(\n",
        "          tf.random.uniform(shape=(label_size, label_size)), trainable=False)\n",
        "    else:\n",
        "      self.transition_params = trans_params\n",
        "      \n",
        "def call(self, inputs, seq_lengths, training=None):\n",
        "  if training is None:\n",
        "    training = K.learning_phase()\n",
        "  if training:\n",
        "    return inputs\n",
        "  return inputs\n",
        "\n",
        "class NerModel(tf.keras.Model):\n",
        "  def __init__(self, hidden_num, vocab_size, label_size, embedding_size, name='BilstmCrfModel', **kwargs):\n",
        "    super(NerModel, self).__init__(name=name, **kwargs)\n",
        "    self.num_hidden = hidden_num\n",
        "    self.vocab_size = vocab_size\n",
        "    self.label_size = label_size\n",
        "    self.embedding = Embedding(vocab_size, embedding_size, mask_zero=True, name='embedding')\n",
        "    self.biLSTM = Bidirectional(LSTM(hidden_num, return_sequences=True), name='bilstm')\n",
        "    self.dense = TimeDistributed(tf.keras.layers.Dense(label_size), name='dense')\n",
        "    self.crf = CRFLayer(self.label_size, name='crf')\n",
        "\n",
        "def call(self, text, labels=None, training=None):\n",
        "  seq_lengths = tf.math.reduce_sum(tf.cast(tf.math.not_equal(text, 0), dtype=tf.int32), axis=-1)\n",
        "  if training is None:\n",
        "    training = K.learning_phase()\n",
        "    inputs = self.embedding(text)\n",
        "    bilstm = self.biLSTM(inputs)\n",
        "    logits = self.dense(bilstm)\n",
        "    outputs = self.crf(logits, seq_lengths, training)\n",
        "    return outputs\n",
        "\n",
        "def loss(self, y_true, y_pred):\n",
        "  y_pred = tf.convert_to_tensor(y_pred)\n",
        "  y_true = tf.cast(self.get_proper_labels(y_true), y_pred.dtype)\n",
        "  seq_lengths = self.get_seq_lengths(y_true)\n",
        "  log_likelihoods, self.transition_params = tfa.text.crf_log_likelihood(y_pred, y_true, seq_lengths)\n",
        "  self.transitionparams = tf.Variable(self.transition_params, trainable=False)\n",
        "  loss= -tf.reduce_mean(log_likelihoods)\n",
        "  return loss\n",
        "\n",
        "def get_proper_labels(self, y_true):\n",
        "  shape = y_true.shape\n",
        "  if len(shape)>2:\n",
        "    return tf.argmax(y_true, -1, output_type=tf.int32)\n",
        "  return y_true\n",
        "\n",
        "def get_seq_lengths(self, matrix):\n",
        "  mask = tf.not_equal(matrix, self.mask_id)\n",
        "  seq_lengths = tf.math.reduce_sum(tf.cast(mask, dtype=tf.int32), axis=-1)\n",
        "  return seq_lengths\n",
        "\n",
        "\n",
        "vocab_size = len(text_vocab) + 1\n",
        "embedding_dim = 64\n",
        "rnn_units = 100\n",
        "BATCH_SIZE = 90\n",
        "num_classes = len(ner_vocab) + 1\n",
        "blc_model = NerModel(rnn_units, vocab_size, num_classes, embedding_dim, dynamic=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "total_sentences = 62010\n",
        "test_size = round(total_sentences / BATCH_SIZE * 0.2)\n",
        "X_train = x_pad[BATCH_SIZE*test_size:]\n",
        "Y_train = Y[BATCH_SIZE*test_size:]\n",
        "X_test = x_pad[0:BATCH_SIZE*test_size]\n",
        "Y_test = Y[0:BATCH_SIZE*test_size]\n",
        "Y_train_int = tf.cast(Y_train, dtype=tf.int32)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train_int))\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "loss_metric = tf.kears.metrics.Mean()\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "  print('Start of epoch %d' (epoch,))\n",
        "  for step, (text_batch, labels_batch) in enumerate(train_dataset):\n",
        "    labels_max = tf.argmax(labels_batch, -1, output_type=tf.int32)\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = blc_model(text_batch, trainint=True)\n",
        "      loss = blc_model.crf.loss(labels_max, logits)\n",
        "      grads = tape.gradient(loss, blc_model.trainable_weights)\n",
        "      optimizer.apply_gradients(zip(grads, blc_model.trainnable_weights))\n",
        "      loss_metric(loss)\n",
        "    if step % 50 == 0:\n",
        "      print('step %s: mean loss = %s' % (step, loss_metric.result()))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}