{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learning_BERT",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOvhptNgknYT/cMlFER8gmY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kk412027247/nlp/blob/main/learning_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI_kGYiAJkdJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "imdb_train, ds_info = tfds.load(name='imdb_reviews', split='train', with_info=True, as_supervised=True)\n",
        "imdb_test = tfds.load(name='imdb_reviews', split='test', as_supervised=True)\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "!pip install transformers==3.0.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tfds.deprecated.text.Tokenizer()\n",
        "vocabulary_set = set()\n",
        "MAX_TOKENS = 0;\n",
        "for example, label in imdb_train:\n",
        "  some_tokens = tokenizer.tokenize(example.numpy())\n",
        "  if MAX_TOKENS < len(some_tokens):\n",
        "    MAX_TOKENS = len(some_tokens)\n",
        "  vocabulary_set.update(some_tokens)\n",
        "imdb_encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary_set, lowercase=True, tokenizer=tokenizer)\n",
        "vocab_size = imdb_encoder.vocab_size\n",
        "print(vocab_size, MAX_TOKENS)"
      ],
      "metadata": {
        "id": "86zhanJIKtHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "def encode_pad_transform(sample):\n",
        "    encoded = imdb_encoder.encode(sample.numpy())\n",
        "    pad = sequence.pad_sequences([encoded], padding='post', \n",
        "                                 maxlen=150)\n",
        "    return np.array(pad[0], dtype=np.int64)  \n",
        "\n",
        "\n",
        "def encode_tf_fn(sample, label):\n",
        "    encoded = tf.py_function(encode_pad_transform, \n",
        "                                       inp=[sample], \n",
        "                                       Tout=(tf.int64))\n",
        "    encoded.set_shape([None])\n",
        "    label.set_shape([])\n",
        "    return encoded, label\n",
        "\n",
        "encoded_train = imdb_train.map(encode_tf_fn,\n",
        "                               num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "encoded_test = imdb_test.map(encode_tf_fn,\n",
        "                             num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "jGxCT1lsU0dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_w2v = {}\n",
        "with open('glove.6B.50d.txt', \"r\") as file:\n",
        "    for line in file:\n",
        "        tokens = line.split()\n",
        "        word = tokens[0]\n",
        "        vector = np.array(tokens[1:], dtype=np.float32)\n",
        "\n",
        "        if vector.shape[0] == 50:\n",
        "            dict_w2v[word] = vector\n",
        "        else:\n",
        "            print(\"There was an issue with \" + word)\n",
        "\n",
        "# lets check the vocabulary size\n",
        "print(\"Dictionary Size: \", len(dict_w2v))"
      ],
      "metadata": {
        "id": "dSVoM8vjnOh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 50\n",
        "embedding_matrix = np.zeros((imdb_encoder.vocab_size, embedding_dim))"
      ],
      "metadata": {
        "id": "gyKM2OubncEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unk_cnt = 0\n",
        "unk_set = set()\n",
        "for word in imdb_encoder.tokens:\n",
        "    embedding_vector = dict_w2v.get(word)\n",
        "\n",
        "    if embedding_vector is not None:\n",
        "        tkn_id = imdb_encoder.encode(word)[0]\n",
        "        embedding_matrix[tkn_id] = embedding_vector\n",
        "    else:\n",
        "        unk_cnt += 1\n",
        "        unk_set.add(word)\n",
        "\n",
        "# Print how many werent found\n",
        "print(\"Total unknown words: \", unk_cnt)"
      ],
      "metadata": {
        "id": "v-NrB45RngCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = imdb_encoder.vocab_size # len(chars)\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 64\n",
        "\n",
        "#batch size\n",
        "BATCH_SIZE=100"
      ],
      "metadata": {
        "id": "YNXy1QtsnjoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, \\\n",
        "                                    Bidirectional, Dense,\\\n",
        "                                    Dropout\n",
        "            \n",
        "def build_model_bilstm(vocab_size, embedding_dim, \n",
        "                       rnn_units, batch_size, train_emb=False):\n",
        "  model = tf.keras.Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
        "              weights=[embedding_matrix], trainable=train_emb),\n",
        "    #Dropout(0.25),  \n",
        "    Bidirectional(tf.keras.layers.LSTM(rnn_units, return_sequences=True, \n",
        "                                      dropout=0.5)),\n",
        "    Bidirectional(tf.keras.layers.LSTM(rnn_units, dropout=0.25)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "ad-Hzwm9nmHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fe = build_model_bilstm(\n",
        "  vocab_size = vocab_size,\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  train_emb=True)\n",
        "\n",
        "model_fe.summary()"
      ],
      "metadata": {
        "id": "NQdfeVUTnoPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fe.compile(loss='binary_crossentropy', \n",
        "             optimizer='adam', \n",
        "             metrics=['accuracy', 'Precision', 'Recall'])"
      ],
      "metadata": {
        "id": "54LZE8Otnqk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)"
      ],
      "metadata": {
        "id": "1ZJG6CnxnsEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_fe.fit(encoded_train_batched, epochs=10)"
      ],
      "metadata": {
        "id": "5ED08q6Dntct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_fe.evaluate(encoded_test.batch(BATCH_SIZE))"
      ],
      "metadata": {
        "id": "IverGJaeszwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "bert_name = 'bert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_name, add_special_tokens=True, do_lower_case=False, max_length=150, pad_to_max_length=True)"
      ],
      "metadata": {
        "id": "JmSmMDSs2t2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode_plus(\"Don't be lured\", add_special_tokens=True, max_length=9,truncation=True, pad_to_max_length=True,return_attention_mask=True, return_token_type_ids=True)"
      ],
      "metadata": {
        "id": "AW9zV72SDLtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode_plus(\"Don't be\", \" lured\", add_special_tokens=True, max_length=10, truncation=True,pad_to_max_length=True,return_attention_mask=True, return_token_type_ids=True)\n"
      ],
      "metadata": {
        "id": "Gk7CJbYKE72K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_encoder(review):\n",
        "  txt = review.numpy().decode('utf-8')\n",
        "  encoded = tokenizer.encode_plus(txt, add_special_tokens=True, max_length=150, truncation=True, pad_to_max_length=True, return_attention_mask=True, return_token_type_ids=True)\n",
        "  return encoded['input_ids'], encoded['token_type_ids'], encoded['attention_mask']\n",
        "\n",
        "bert_train = [bert_encoder(r) for r, l in imdb_train]\n",
        "bert_lbl = [l for r, l in imdb_train]\n",
        "bert_train = np.array(bert_train)\n",
        "bert_lbl = tf.keras.utils.to_categorical(bert_lbl, num_classes=2)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(bert_train, bert_lbl, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "IobI3Y1MFXPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape, y_train.shape)"
      ],
      "metadata": {
        "id": "PZx5w9d-c5w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_reviews, tr_segments, tr_masks = np.split(x_train, 3, axis=1)\n",
        "val_reviews, val_segments, val_masks = np.split(x_val, 3, axis =1)\n",
        "tr_reviews = tr_reviews.squeeze()\n",
        "tr_segments = tr_segments.squeeze()\n",
        "tr_masks = tr_masks.squeeze()\n",
        "val_reviews=val_reviews.squeeze()\n",
        "val_segments=val_segments.squeeze()\n",
        "val_masks = val_masks.squeeze()\n",
        "\n",
        "def example_to_features(input_ids, attention_mask, token_type_ids, y):\n",
        "  return {'input_ids': input_ids, 'attention_mask':attention_mask,'token_type_ids':token_type_ids }, y\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((tr_reviews, tr_masks, tr_segments, y_train)).map(example_to_features).shuffle(100).batch(16)\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((val_reviews, val_masks, val_segments, y_val)).map(example_to_features).shuffle(100).batch(16)\n",
        "\n",
        "from transformers import TFBertForSequenceClassification\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained(bert_name)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "bert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "bert_model.summary()"
      ],
      "metadata": {
        "id": "lEGmxh_6dWL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bert_history = bert_model.fit(train_ds, epochs=3, validation_data=valid_ds)"
      ],
      "metadata": {
        "id": "5EoYU-TCxI-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_test =[bert_encoder(r) for r, l in imdb_test]\n",
        "bert_tst_lbl = [l for r, l in imdb_test]\n",
        "bert_test2 = np.array(bert_test)\n",
        "bert_tst_lbl2 = tf.keras.utils.to_categorical(bert_tst_lbl, num_classes=2)\n",
        "\n",
        "ts_reviews, ts_segments, ts_masks = np.split(bert_test2, 3, axis=1)\n",
        "ts_reviews = ts_reviews.squeeze()\n",
        "ts_segments = ts_segments.squeeze()\n",
        "ts_masks = ts_masks.squeeze()\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((ts_reviews, ts_segments, ts_masks, bert_tst_lbl2)).map(example_to_features).shuffle(100).batch(16)"
      ],
      "metadata": {
        "id": "6RyDOERH1g4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bert_model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "28ofDZxz3yXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertModel\n",
        "bert_name = 'bert-base-cased'\n",
        "bert = TFBertModel.from_pretrained(bert_name)\n",
        "bert.summary()"
      ],
      "metadata": {
        "id": "PUdSQVGU6QjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 150\n",
        "inp_ids = tf.keras.layers.Input((max_seq_len,), dtype=tf.int64, name='input_ids')\n",
        "att_mask = tf.keras.layers.Input((max_seq_len,), dtype=tf.int64, name='attention_mask')\n",
        "seg_ids = tf.keras.layers.Input((max_seq_len,), dtype=tf.int64, name='token_type_ids')\n",
        "\n",
        "train_ds.element_spec\n",
        "\n",
        "inp_dict = {'input_ids':inp_ids, 'attention_mask':att_mask, 'token_type_ids':seg_ids}\n",
        "outputs = bert(inp_dict)\n",
        "outputs\n",
        "\n",
        "x = tf.keras.layers.Dropout(0.2)(outputs[1])\n",
        "x = tf.keras.layers.Dense(200, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = tf.keras.layers.Dense(2, activation='sigmoid')(x)\n",
        "custom_model = tf.keras.models.Model(inputs=inp_dict, outputs=x)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "custom_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "custom_model.summary()"
      ],
      "metadata": {
        "id": "k8Tx2cDw9zfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_history = custom_model.fit(train_ds, epochs=2, validation_data=valid_ds)"
      ],
      "metadata": {
        "id": "hYcULfrrEJff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "LoG2TEzBERrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert.trainable=False\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "custom_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "_KOINabcGoQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_model.summary()"
      ],
      "metadata": {
        "id": "zWS6sXn-Hl0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_history = custom_model.fit(train_ds, epochs=10, validation_data=valid_ds)"
      ],
      "metadata": {
        "id": "XiSyz8W4HznZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}