{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learning_BERT",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM4tlEJYuBR2iP8tM+LSw6e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kk412027247/nlp/blob/main/learning_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI_kGYiAJkdJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "imdb_train, ds_info = tfds.load(name='imdb_reviews', split='train', with_info=True, as_supervised=True)\n",
        "imdb_test = tfds.load(name='imdb_reviews', split='test', as_supervised=True)\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tfds.deprecated.text.Tokenizer()\n",
        "vocabulary_set = set()\n",
        "MAX_TOKENS = 0;\n",
        "for example, label in imdb_train:\n",
        "  some_tokens = tokenizer.tokenize(example.numpy())\n",
        "  if MAX_TOKENS < len(some_tokens):\n",
        "    MAX_TOKENS = len(some_tokens)\n",
        "  vocabulary_set.update(some_tokens)\n",
        "imdb_encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary_set, lowercase=True, tokenizer=tokenizer)\n",
        "vocab_size = imdb_encoder.vocab_size\n",
        "print(vocab_size, MAX_TOKENS)"
      ],
      "metadata": {
        "id": "86zhanJIKtHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "def encode_pad_transform(sample):\n",
        "    encoded = imdb_encoder.encode(sample.numpy())\n",
        "    pad = sequence.pad_sequences([encoded], padding='post', \n",
        "                                 maxlen=150)\n",
        "    return np.array(pad[0], dtype=np.int64)  \n",
        "\n",
        "\n",
        "def encode_tf_fn(sample, label):\n",
        "    encoded = tf.py_function(encode_pad_transform, \n",
        "                                       inp=[sample], \n",
        "                                       Tout=(tf.int64))\n",
        "    encoded.set_shape([None])\n",
        "    label.set_shape([])\n",
        "    return encoded, label\n",
        "\n",
        "encoded_train = imdb_train.map(encode_tf_fn,\n",
        "                               num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "encoded_test = imdb_test.map(encode_tf_fn,\n",
        "                             num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "jGxCT1lsU0dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_w2v = {}\n",
        "with open('glove.6B.50d.txt', \"r\") as file:\n",
        "    for line in file:\n",
        "        tokens = line.split()\n",
        "        word = tokens[0]\n",
        "        vector = np.array(tokens[1:], dtype=np.float32)\n",
        "\n",
        "        if vector.shape[0] == 50:\n",
        "            dict_w2v[word] = vector\n",
        "        else:\n",
        "            print(\"There was an issue with \" + word)\n",
        "\n",
        "# lets check the vocabulary size\n",
        "print(\"Dictionary Size: \", len(dict_w2v))"
      ],
      "metadata": {
        "id": "dSVoM8vjnOh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 50\n",
        "embedding_matrix = np.zeros((imdb_encoder.vocab_size, embedding_dim))"
      ],
      "metadata": {
        "id": "gyKM2OubncEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unk_cnt = 0\n",
        "unk_set = set()\n",
        "for word in imdb_encoder.tokens:\n",
        "    embedding_vector = dict_w2v.get(word)\n",
        "\n",
        "    if embedding_vector is not None:\n",
        "        tkn_id = imdb_encoder.encode(word)[0]\n",
        "        embedding_matrix[tkn_id] = embedding_vector\n",
        "    else:\n",
        "        unk_cnt += 1\n",
        "        unk_set.add(word)\n",
        "\n",
        "# Print how many werent found\n",
        "print(\"Total unknown words: \", unk_cnt)"
      ],
      "metadata": {
        "id": "v-NrB45RngCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = imdb_encoder.vocab_size # len(chars)\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 64\n",
        "\n",
        "#batch size\n",
        "BATCH_SIZE=100"
      ],
      "metadata": {
        "id": "YNXy1QtsnjoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, \\\n",
        "                                    Bidirectional, Dense,\\\n",
        "                                    Dropout\n",
        "            \n",
        "def build_model_bilstm(vocab_size, embedding_dim, \n",
        "                       rnn_units, batch_size, train_emb=False):\n",
        "  model = tf.keras.Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
        "              weights=[embedding_matrix], trainable=train_emb),\n",
        "    #Dropout(0.25),  \n",
        "    Bidirectional(tf.keras.layers.LSTM(rnn_units, return_sequences=True, \n",
        "                                      dropout=0.5)),\n",
        "    Bidirectional(tf.keras.layers.LSTM(rnn_units, dropout=0.25)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "ad-Hzwm9nmHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fe = build_model_bilstm(\n",
        "  vocab_size = vocab_size,\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  train_emb=True)\n",
        "\n",
        "model_fe.summary()"
      ],
      "metadata": {
        "id": "NQdfeVUTnoPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fe.compile(loss='binary_crossentropy', \n",
        "             optimizer='adam', \n",
        "             metrics=['accuracy', 'Precision', 'Recall'])"
      ],
      "metadata": {
        "id": "54LZE8Otnqk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)"
      ],
      "metadata": {
        "id": "1ZJG6CnxnsEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fe.fit(encoded_train_batched, epochs=30)"
      ],
      "metadata": {
        "id": "5ED08q6Dntct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fe.evaluate(encoded_test.batch(BATCH_SIZE))"
      ],
      "metadata": {
        "id": "IverGJaeszwK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}